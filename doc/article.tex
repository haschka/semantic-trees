\documentclass[10pt,a4paper,twocolumn]{article}

\usepackage{graphicx}
\usepackage{hyperref}

\author {
  Thomas Haschka \and Joseph Bakarji
  }

\title{Semantic Tree Inference on Text Corpa using a Nested Density
  Approach together with Large Language Model Embeddings}

\begin{document}
\maketitle

\section*{Abstract}

Semantic text classification has undergone significant advances in
recent years due to the rise of large language models (LLMs) and their
high dimensional embeddings. While LLM-embeddings are frequently used to store
and retrieve text by semantic similarity in vector databases, the
global structure semantic relationships in text corpora often remains
opaque.

Herein we propose a nested density clustering
approach, to infer hierarchical trees of semantically related texts. The
method starts by identifying texts of strong semantic similarity as it searches
for dense clusters in LLM embedding space. As the density
criterion is gradually relaxed, these dense
clusters merge into more diffuse clusters, until the whole dataset is
represented by a single cluster - the root of the tree. By embedding
dense clusters into increasingly diffuse ones, we construct a tree
structure that captures hierarchical semantic relationships among texts.

We outline how this approach can be used to classify textual data for
abstracts of scientific abstracts as a case study. This enables the
data-driven discovery research areas and their subfields without
predefined categories. To evaluate the general applicability of the
method, we further apply it to established benchmark datasets such as
the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its
robustness across domains. 

Finally we discuss possible applications on scientometrics, topic
evolution, highlighting how nested density trees can reveal semantic
structure and evolution in textual datasets. 

\section{Background}

Text classifications date back at least to bag of words models
\cite{bagofwords}. In these early approaches, each text is represented
as a vector of word frequencies, disregarding the order of words.
However, word order is a key semantic feature as changing it can
completely alter the meaning of a phrase. 

Subsequent refinements introduced term frequency and inverse document
frequency (tf-idf)
normalization \cite{tfidf}, which weights words by their relative
importance within a corpus. Later, Latent
Semantic Analysis (LSA) \cite{lsa-first,lsa,lsa-intro} applied
singlar value decomposition to capture latent relationships among
words and documents, enabling more robust document clustering and
topic modeling.
Easier to scale and based on probabilistic properties
instead of linear algebra Latent Dirichlet Allocation (LDA) \cite{lda}
and Hirachical Dirichlet Allocation (HDA) \cite{hda} have overtaken
the field of topic modelling. Neural network based approaches
such as word2vec \cite{word2vec} have arrived in the middle of the
last decade. These models could learn the relationships between words
in a semantic proximity sense, but still not in a grammatical
sequence.
The rise of the transformer model \cite{transformer} and the popularization
of LLMs allowed for the use of LLM embeddings for document
classification, and document retrieval. The transformer model is
superieur as it efficiently encodes and uses
the position of the words in the text as it generates embeddings. 
LLM embeddings have a such a vast field of applications and
\cite{llm-embed-chatbot,llm-embed-clust} commonly found in retrieval
augmented generation (RAG) pipelines \cite{rag,llm-embed-rag}.
All of the aforementioned methods project text into a mathematical
embedding space, where pairwise distances and clustering can be
applied to reveal latent semantic structures.
In parallel, hierarchical density clustering techniques have proven
highly effective in fields such as biological species classification,
phylogenetics \cite{mnhn-tree-tools}, and molecular dynamics, where
they are used to analyze protein conformations and structural
evolution \cite{md-tree-tools}.

Here, we extend these hierarchical density clustering methods to
textual embeddings, constructing tree-based representations of
semantically related texts. 
This approach goes beyond flat clustering by revealing how topics,
subjects, or emotions (depending on the embedding model) relate to one
another within a coherent hierarchical framework. 

\section{Introduction}

Topic modeling and semantic clustering have long aimed to uncover the
latent structure of large text corpora. With the rise of large
language models (LLMs) and their embeddings, it has become possible to
represent textual semantics with unprecedented precision. However,
most embedding-based clustering approaches produce flat partitions of
the semantic space, providing limited insight into how topics or
concepts are hierarchically related or evolve across scales.

In this work, we introduce a hierarchical density-based clustering
approach that infers tree structures from LLM embedding spaces. The
method builds upon previous applications of hierarchical density
clustering in bioinformatics and molecular dynamics, where it has been
successfully used to construct phylogenetic or conformational trees
from complex, high-dimensional data \cite{mnhn-tree-tools,
  md-tree-tools}. By adapting this framework to text embeddings, we
enable the reconstruction of semantic hierarchies. We show how dense
clusters of highly similar documents merge into broader, more diffuse
clusters as the similarity threshold is relaxed.

To construct such hierarchical structures, we employ a nested
density-based clustering procedure inspired by the DBSCAN algorithm
\cite{dbscan}. The core idea is to identify compact, high-density
regions of semantically similar embeddings and then progressively
relax the density criterion to reveal broader, more diffuse
groupings. This iterative relaxation enables clusters discovered at a
high-density level to merge into larger clusters at lower density
levels, naturally producing a hierarchical tree.
Formally, clusters of semantically similar texts that that
are densly connected in their respective embedding space are
determined by the condition:
\begin{equation}
  \rho_L > \frac{\mathrm{minpts}}{V(\epsilon_L)},
  \label{eqn-density-criterium}
\end{equation}
where $\rho_L$ represents the denstiy at a given iteration $L$. The
right hand side is given by starting parameters to the well known
Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
algorithm \cite{dbscan}, with $\epsilon_L$ defining a radius at
iteration $L$ and $\mathrm{minpts}$ a minimum number of points within
this radius, effectively defining the density of a continious dense
ensemble to be found by the algorithm. To explore the semantic
structure at multiple scales, the radius $\epsilon_L$ is incrementally
expanded by a small step $\Delta\epsilon$ in each iteration. This
results in a sequence of density thresholds:  As $\epsilon$
increases, the volume in equation (\ref{eqn-density-criterium}) expands
leading to lower densities and
the identification of more diffuse clusters in each iteration. These
new clusters encapsulate denser ones from previous iterations,
allowing us to embed fine-grained semantic clusters into coarser ones,
thereby forming a hierarchical tree of semantic relationships. For a
subsequent layer (iteration) of the tree $L+1$ it follows, equation
(\ref{eqn-layers}):
\begin{equation}
  \rho_{L+1} > \frac{\mathrm{minpts}}{V(\epsilon_L+\Delta\epsilon)},
  \label{eqn-layers}
\end{equation}
or alternatively $\epsilon_L = \epsilon_{L-1} + \Delta \epsilon$. In
our previous work \cite{mnhn-tree-tools}, we demonstrated that using
this algorithm allows us to construct a hierarchical tree structure by
embedding dense clusters into progressively more diffuse clusters
until all elements of the conformational space merge into a single
cluster - the root of the tree. Figure
\ref{fig-tree-from-space} outlines the tree building procedure
conceptually. On the left, a text embedding space is shown, where each
colored sphere represents a piece of text. On the right the tree built
by the algorithm, from the text embedding space on the left, is shown. One can
observe how the grape, and orange colored texts fusion as the
density criteria is relaxed into the blue ensemble, encompassing both
clusters. At an intermediate step the green cluster stays separate,
while the violet texts are not encompassed and seen as outliers. Finally, as the density criterion is further relaxed all texts are only part of one cluster, represented in gray on the right tree-outline. 
\begin{figure*}
  \includegraphics{tree-from-space.pdf}
  \caption{Tree building from a dataset represented by its embedding
    space. On the left, an embedding space with clusters of different densities is shown. One can see that the orange, and grape colored clusters embed into a larger blue ensemble. A conceptual tree
    built from such a two dimensional embedding space is shown on the right.}
  \label{fig-tree-from-space}
\end{figure*}
Our algorithm allows an organization for instance to show how
documents within the organization are related. It initially finds separated clusters of highly semantically similar documents (the outer leafs of the tree), that merge, layer
by layer together to more diffuse clusters, until all documents form a
single cluster (the root of the tree).

We demonstrate our approach on several corpora, including collections
of scientific abstracts and benchmark datasets such as 20 Newsgroups
and IMDB 50K reviews. In these experiments, we show how the inferred
semantic trees capture both fine-grained clusters of related documents
and higher-level thematic organization. Applied to the publication
records of institutions such as TU Wien and the American University of
Beirut, our method reveals naturally emerging research areas and
structural relationships between topics.

Once a semantic tree is constructed, an important challenge lies in
interpreting and annotating its clusters. We address this by
leveraging LLMs themselves: by prompting a model with the text content
of each cluster, we can automatically generate concise, human-readable
summaries or titles for each node in the hierarchy. This creates an
interpretable and fully automated pipeline for semantic structure
discovery in large text corpora.

\section{Materials and Methods}

\subsection{Datasets} \label{sec-datasets}

The primary goal of this study was to develop a pipeline for the
automated classification of research fields based on textual
similarity. While motivated by this scientific application, the
proposed approach is general and can be applied to a broad range of
text corpora. To ensure linguistic consistency and to avoid potential
errors caused by multilingual embeddings and the tree building
process, which would in such a case probably attributed different
branches to different languages, we restricted our analysis
to texts written in English.

We selected the following datasets for our study:
\begin{itemize}
  \item \textbf{AUB}: English abstracts of publications authored by the
    American University of Beirut (AUB) from 2018 to October 2025. The data
    were provided by the AUB University Libraries. To ensure comparability,
    the roughly the same temporal window and language filters, only english
    abstracts, as for the TU Wien corpus were applied.
  \item \textbf{TU Wien}:   English abstracts of publications affiliated
    with TU Wien between 2018 and April 2025. The data were provided by
    the TU Wien Service Unit for Research Information Systems as
    aggregated extracts from the institutional repository
    (\emph{reposiTUm}), Scopus, and Dimensions databases. Duplicates
    were removed, and only records containing English abstracts were
    retained. 
  \item \textbf{20 Newsgroups}: The 20 Newsgroups dataset is a widely
    used benchmark corpus for 
    text classification and topic modeling. It consists of
    approximately 20,000 newsgroup posts collected from 20 distinct
    discussion groups on topics such as science, politics, sports,
    religion, and technology \cite{20newsgroups}.
  \item \textbf{IMDB 50K Reviews}:
     A corpus of 50,000 movie reviews from
     the Internet Movie Database (IMDB), each labeled with a binary
     sentiment (positive or negative) \cite{50kmovies}. 
   \item \textbf{AG News}: A large-scale news dataset consisting of short
     article titles and descriptions categorized into four topics: World,
     Sports, Business, and Science/Technology.
     \cite{agnews}.
   \item \textbf{Theses-fr-en}: English abstracts of theses defended in
    France: The French
    government publishes by means of its open data initiative a
    dataset containing all theses defended in France since 1985. We
    filtered this dataset for english abstracts. This data was
    obtained from the \emph{data.gouv.fr} platform.
\end{itemize}
Statistics about the different datasets are shown in table
\ref{tab-dataset-statistics}.

\begin{table}
  \begin{center}
  \begin{tabular}{l|rrr}
    & n(Texts) & mean & stddev \\
    \hline
    TU Wien & 27506 & 168.85 & 73.79 \\
    AUB & 15982 & 237.73 & 98.24 \\
    20newsgroups & 11314 & 287.47 & 541.46 \\
    50k-movies & 50000 & 231.16 & 171.34 \\
    ag\_news & 127000 & 37.84 & 10.09 \\
    theses-fr-en & 180939 & 261.89 & 112.70 \\
    \end{tabular}
  \caption{Statistics about the text corpa used. \emph{n(Texts)} is
    the number of unique texts in the dataset, \emph{mean} and
    \emph{stddev} represents the mean and standard deviation of the
    number of words contained in each text of the dataset.}
  \label{tab-dataset-statistics}
  \end{center}
\end{table}
    
\subsection{Embeddings}
Embeddings for the datasets outlined in section \ref{sec-datasets}
were generated using either the Qwen3-Embedding-8B
\cite{qwen3embedding} or SFR-Embedding-Mistral \cite{SFRembedding}
model. These models were chosen selected based on their high performance in the
MTEB-Leaderboard found on Huggingface, and the their practical
deployability on the computing infrastructure accessible to us.

4096 dimensonal embedding vectors obtained for each text from a
dataset were stored in an in house developed vector database.

Each text sample was transformed into a 4096-dimensional embedding
vector. The resulting embeddings were stored in an in-house developed
vector database optimized for high-throughput semantic similarity
retrieval.

\subsection{Tree Building}

\subsubsection{Pairwise Distance and Dimensionality Reduction}

The pairwise distance between embeddings serves as the foundation for
the hierarchical tree construction process. We employed the cosine
distance on the full-length embedding vectors as well as the $L_2$-norm
on reduced-dimensional representations obtained via Principal
Component Analysis (PCA). 

Dimensionality reduction proved crucial, as PCA functions as a feature
selection mechanism that prioritizes components with maximum variance.
Empirically, we observed that reducing the number of components leads
to more clearly separated and interpretable cluster structures. While
the original embedding vectors contain 4096 dimensions, spectral
analysis indicated significant redundancy among components. The choice
of the number of principal components thus directly affects the
resolution and spread of the resulting semantic tree.

\subsubsection{Adaptive Density Clustering with DBSCAN}

For each dataset, we constructed trees using an adaptive density-based
clustering procedure built upon successive runs of the DBSCAN
algorithm. Each DBSCAN iteration identifies clusters at a specific
density level, controlled by a triplet of parameters: the initial
neighborhood radius $\epsilon_0$, the incremental radius step
$\Delta\epsilon$, and the minimum number of points within the radius
($\mathrm{minpts}$). 

In each successive run, the radius is increased by $\Delta\epsilon$
while keeping $\mathrm{minpts}$ constant. This progressively lowers
the effective density threshold and allows previously dense clusters
to merge into more diffuse ones. Whenever the number of clusters found
in a given iteration is smaller than in the preceding step, the newly
identified clusters are stored, and the clusters from the prior
iteration are embedded within them. This iterative embedding process
builds the hierarchical tree structure described in
Figure \ref{fig-tree-from-space}, where dense clusters at outer layers
merge stepwise into broader, semantically diffuse clusters toward the
root.

\subsection{Tree Annotation}

The resulting trees consist of multiple hierarchical layers. Each
layer represents a set of clusters that form the nodes at that level,
while edges connect clusters of higher density (outer layers) to
broader, lower-density clusters (inner layers). This structure allows
fine-grained semantic groups to merge progressively into coarser
domains.

To realize our goal of a data-driven research field classification
pipeline, we sought to automatically annotate each node with a
representative topic label. The underlying assumption is that smaller,
high-density clusters correspond to specialized research areas, while
broader clusters capture more general scientific domains.

To generate these annotations, we employed a Large Language Model
(LLM) capable of handling large textual contexts. Specifically, we
used the Llama-4-Scout-17B-16E-Instruct model
\cite{meta2024llama4} in 6-bit quantized form,
which provides a nominal context window of up to
11 million tokens. Due to technical constraints, our inference
pipeline successfully operated with a maximum context size of
approximately 600,000 tokens.

Both the TU Wien and AUB corpora were fully annotated, encompassing
all English-language scientific abstracts published since 2018. For
each node, the LLM received the text of all abstracts within that
cluster and was prompted to infer a unifying research field label. 

In cases where a cluster contained more than 1500 abstracts (exceeding
the 600,000-token limit), we randomly sampled 1500 abstracts per
processing chunk. For $n$ total abstracts in a cluster, the number of
chunks $s$ was determined as:$
  s = \frac{3n}{1500}$,
where the factor of 3 ensures oversampling to enhance topic stability.
Each chunk was processed independently by the LLM to yield a candidate
research field label. In less then 1 in 100 cases, the LLM did not
produce a clear output. Even though the prompt enforces to yield only
one research field on a single line, this is not always the case. As
such in all cases where answers span more than one line, or contain
more than 80 characters, we
yield these answers to the gpt-oss-20b model \cite{gpt-oss} in 8-bit
quantized form, and ask it
to decide from the previous output on a single research field. As the
output from the first run might be due to errors in the LLM very large
we only keep the last 20000 characters during this pass to the
gpt-oss-20b model. 

In the case were the cluster had to be sampled because it contained
more than 1500 abstracts all resulting labels were subsequently fed into
a second LLM pass. Again the gpt-oss-20b model \cite{gpt-oss} in 8-bit
quantized form is used in such a case where the model inferred the
overarching research field encompassing all $s$ subfields. An overview about
the different paths that annotation might undergo is outlined in Figure
\ref{fig-annotation}.
\begin{figure}
  \begin{center}
    \includegraphics{annotation-schema.pdf}
    \caption{Shematic represantation of the tree node / cluster
      annotation process}
    \label{fig-annotation}
  \end{center}
\end{figure}
  

Tree annotation is computationally intensive. The AUB tree required
annotating 33{,}862 text files, while the TU Wien tree required
62{,}012, including oversampling. Inference was performed using the
\texttt{llama.cpp} \cite{llama-cpp} engine on the MUSICA supercomputer
(Austria). Up to 80 NVIDIA H100 GPUs were employed concurrently during
annotation. Each compute node with four H100 GPUs (80~GB VRAM each)
handled batches of approximately 600{,}000 tokens per inference
instance. Multi-node inference, which could allow for a higher context
token limit, was not attempted as MUSICA is currently in its test
phase; instead, parallelism was achieved by distributing text files
across multiple nodes for simultaneous inference.

\subsection{Label Transcription} \label{sec-transcription}

The theses-fr-en dataset is annotated and contains a research field
(discipline, in French) for each abstract. However, these fields can
in many cases be freely chosen by the thesis author, resulting in 13,599
unique discipline labels.
In a first test we cross annotated these unique discipline labels into
a binary classification of either humanities or natural sciences. We
then proceeded to obtain a more coherent and standardized annotation,
we used the gpt-oss-20b model to classify all 13,599 labels according to the
OECD Fields of Science (FOS) 2002 classification \cite{oecd-fos}. 
The resulting transcribed labels were then used to color
the trees inferred from this dataset, allowing us to assess whether
specific scientific fields tend to separate into distinct
and meaningful branches. 

\subsection{Tree Coloration and Visualization}

To facilitate interpretation and qualitative validation of the inferred
trees, we applied coloration schemes that project existing labels or
metadata onto the tree structure. This process enables us to visualize
how known categories or attributes distribute across different branches
of the tree.

For example, in the case of the 20~Newsgroups dataset, tree
coloration reveals how posts from individual newsgroups are positioned
within the hierarchical semantic structure. Similarly, for the
TU~Wien dataset, coloration based on existing research
classifications allowed us to assess how well our hierarchical density
clustering recovers semantically coherent domains.

Tree coloration was performed using utilities provided by
MNHN-Tree-Tools \cite{mnhn-tree-tools}. In particular, we
employed the \texttt{tree\_map\_for\_split\_set} tool to map predefined
clusters or labeled subsets onto the corresponding nodes of the
hierarchical tree. The resulting color maps were generated for
visualization using the newick-utilities package
\cite{newick-utilities}, a software suite for the manipulation and
visualization of phylogenetic trees.

This approach allowed us to produce colored tree visualizations for
each dataset. In the 20 Newsgroups case, individual trees were
colored according to each newsgroup category, highlighting topic
segregation and overlap. For the IMDB 50K reviews dataset, we colored
the tree by sentiment polarity, enabling a visual inspection of how
positive and negative reviews cluster across different density layers.

For the annotated datasets from TU Wien and AUB, we required a
high-resolution visualization that allows all node-level labels to be
displayed and inspected. The resulting tree images are extremely large,
making standard static rendering impractical. To enable detailed
exploration, we developed a tile-based viewer application with two
coordinated windows. One window displays the full, non-annotated tree,
while the second window shows a zoomed view of a selected region,
including the labels for the nodes currently in focus. Navigation is
performed either by clicking on the full-tree view, which updates the
zoomed window to the selected location, or by dragging within the zoomed
window, which moves a red cursor on the full-tree view to indicate the
corresponding position.

\section{Results}

\subsection{Efficiency of PCA and the $L_2$-norm Compared to Cosine Distance}

Cosine distance is widely used for comparing high-dimensional embedding
vectors and is the default similarity measure in many retrieval-based
applications, including RAG pipelines. For this reason, we initially
expected cosine distance to be the natural choice for the DBSCAN
iterations underlying our tree construction algorithm. However, trees
constructed directly from full 4096-dimensional cosine distances
consistently exhibited limited structural resolution: branches were
poorly separated, fine-grained clusters failed to emerge, and the overall
tree topology did not yield significant insights.

In contrast, applying Principal Component Analysis (PCA) prior to
clustering dramatically improved the quality and expressiveness of the
resulting trees. Experiments with 10, 5, and 2 principal components
showed that lower-dimensional PCA representations consistently produced
more “spread out” trees with clearer branching and more detectable
fine-scale semantic substructure. We suggest that this effect is due
to the inherent ability of PCA to yield principal components that
maximize variance and that this feature selection process has a
positive effect on our clustering algorithm. A similar effect was
noted previously in the algorithmic supplement of
\cite{mnhn-tree-tools}.

A comparison between trees obtained with cosine distance and with the
$L_2$-norm applied to PCA-reduced embeddings is shown in Figure
\ref{fig-compairison-L2-cos}. From left to right, the trees become more
hierarchically detailed and reveal progressively finer semantic
subclusters. For these illustrative examples, the parameters were chosen
using a rudimentary bisection search to produce visually optimized
trees. The parameter settings were as follows: for cosine distance,
initial $\epsilon = 0.275$, $\Delta \epsilon = 0.001$, and
$\mathrm{minpts}=5$; for the $L_2$-norm on 10 principal components,
initial $\epsilon = 0.091$, $\Delta \epsilon = 0.0001$,
$\mathrm{minpts}=5$; and for the $L_2$-norm on 2 principal components,
initial $\epsilon = 0.0039$, $\Delta \epsilon = 0.00001$,
$\mathrm{minpts}=5$. The trees shown in \ref{fig-compairison-L2-cos}
were built from embeddings obtained form the Qwen3-Embedding-8B model.
\begin{figure*}
  \begin{center}
  \includegraphics{comparision-cos-L2.pdf}
  \caption{A comparison of trees built from the 20 Newsgroups dataset
    under different distance measures}
  \label{fig-compairison-L2-cos}
  \end{center}
\end{figure*}

\subsection{Detection of Topics and Semantics}

Depending on the dataset, we applied different evaluation strategies to
assess whether, and to what extent, the method proposed in this work
produces meaningful and reliable results, given the computational
constraints of our setup. The following subsections outline these
evaluations and illustrate the behavior of our hierarchical
density-based tree construction across diverse text corpora.

\subsubsection{20 Newsgroups}

To evaluate whether our tree construction method recovers meaningful
semantic structure, we applied it to the 20 Newsgroups dataset. The
resulting trees were colored according to posting labels, as shown in
Figure \ref{fig-20news-colored}. For each newsgroup we generated a
separate color map: posts from the target newsgroup are shown in blue,
and posts from all other newsgroups in orange. Both color intensities
follow a logarithmic scale proportional to the number of posts in each
cluster, with white indicating the absence of posts of a given type.

The inferred tree, as highlighted in figure \ref{fig-20news-colored},
recovers many interesting relationships between the newsgroups:

\begin{itemize}
\item \textbf{Sports-related groups.}  
  The groups \emph{rec.sport.baseball} and \emph{rec.sport.hockey}
  form a shared subtree located close to the root of the full tree.
  This indicates strong internal similarity between the two sports
  discussions, combined with substantial separation from most other
  topics.

\item \textbf{Automotive-related groups.}  
  The groups \emph{rec.autos} and \emph{rec.motorcycles} cluster
  tightly in a small dedicated sub-branch. A fraction of their posts
  appears in regions of the tree that correspond to scientific or
  technical discussions, likely reflecting threads involving
  engineering aspects of vehicles.

\item \textbf{Religion and politics.}  
  The groups \emph{soc.religion.christian}, \emph{alt.atheism}, and
  \emph{talk.religion.misc} form a joint subtree that also contains
  the political groups \emph{talk.politics.guns},
  \emph{talk.politics.mideast}, and \emph{talk.politics.misc}.  This
  intersection reflects the topical proximity and overlapping
  discussions between religion and politics within the dataset.

\item \textbf{Computer-related groups.}  
  The \emph{comp.*} groups, including \emph{comp.sys.mac.hardware},
  \emph{comp.windows.x}, \emph{comp.graphics}, and
  \emph{comp.os.ms-windows}, form a coherent subtree corresponding to
  computer hardware and software discussions. Some overlap with
  \emph{sci.electronics} is visible, consistent with semantically
  similar content.

\item \textbf{Marketplace group.}
  The \emph{misc.forsale} posts distribute across multiple regions of
  the tree, often co-occurring with the automotive and computer
  subtrees, suggesting that sale-related messages tend to mirror the
  underlying topical focus of corresponding technical groups.

\item \textbf{The cryptography group.}  
  The \emph{sci.crypt} group appears in a distinct set of branches
  close to the root, and is clearly separated from both the scientific
  and computer-related subtrees. This separation may stem from the
  unique vocabulary and specialized discourse typical of cryptography
  discussions.
\end{itemize}

Overall the structure seems to intuitively make sense, outlining
intervoven fields of discussion. Importantly,
our toolchain does not merely cluster newsgroups into isolated groups;
it also conveys the relative proximity and semantic distance between
them. The resulting tree therefore serves as a compact visual
instrument that captures both fine-grained cluster structure and the
broader topology of topic similarity within the dataset, all within a
single interpretable representation. 
\begin{figure*}
  \begin{center}
    \includegraphics[scale=0.8]{20-newsgroups.pdf}
    \caption{Representations of the tree obtained from the 20 Newsgroups dataset. Trees were built from the first two principal components obtained from Qwen3-Embedding-8B \cite{qwen3embedding} embeddings. Parameters used to build tree were: initial $\epsilon = 0.0039$, $\Delta\epsilon = 0.00001$, $\mathrm{minpts} = 5$.}
    \label{fig-20news-colored}
  \end{center}
\end{figure*}

\subsubsection{IMDB 50K Reviews}

The IMDB 50K Reviews dataset was used to evaluate whether our tree
construction method, together with the underlying embedding models, is
able to separate reviews by emotional polarity. We therefore built a
tree from this dataset following the same procedure as for the other
corpora. The resulting tree was then colored analogously to the 20
Newsgroups experiments: clusters containing predominantly positive
reviews were assigned a blue shading, while clusters containing
predominantly negative reviews were shaded in orange. As before, color
intensity follows a logarithmic scale, with white indicating the
absence of reviews of a given label and deeper blue or orange
indicating higher concentration. Overlaps appear in darker,
greenish to black tones. 

The resulting tree is shown in Figure
\ref{fig-50k-news-colored}. Compared to the other datasets, the IMDB
tree appears noticeably less structured, suggesting that the
underlying corpus is more homogeneous and lacks strong semantic
subdivisions beyond sentiment. Nonetheless, we observe a few small
subtrees near the periphery of the tree in which positive reviews form
a clear majority. Additional localized regions with a majority of
positive or negative reviews can also be identified. We speculate that
these structures arise from repetitive linguistic patterns that occur
more frequently in either positive or negative reviews. However, they
do not form large coherent branches, and overall the dataset does not
separate cleanly by sentiment. 

This observation suggests that the embedding models
used: Qwen3-Embedding-8B \cite{qwen3embedding} and
SFR-Embedding-Mistral \cite{SFRembedding} encode emotional polarity
only to a limited extent. The results highlight the importance of
choosing embedding models appropriate for the expected semantic
structure when constructing trees. 

We also attempted to annotate the IMDB tree with the
Llama-4-Scout-17B-16E-Instruct model \cite{meta2024llama4}. The goal
was to obtain genre-level descriptions of tree regions, under the
assumption that certain movie genres might correlate with
predominantly positive or negative reviews. This attempt was
unsuccessful: the model did not produce reliable or meaningful genre
assignments, and more fundamentally, such information appears to be
largely absent from the review text itself. As such, deriving movie
genres from this dataset seems infeasible with current methods. 
\begin{figure}
  \begin{center}
    \includegraphics{50-pos-neg.pdf}
    \caption{The IMDB 50K Reviews dataset colored by sentiments: The tree was built using the first two principal components derived from SFR-Embedding-Mistral \cite{SFRembedding} embeddings. Parameters yield to the tree building algorithm were: initial $\epsilon = 0.5$, $\Delta\epsilon = 0.0001$, $\mathrm{minpts} = 5$.}
    \label{fig-50k-news-colored}
  \end{center}
\end{figure}

\subsubsection{AG News}

We further built trees from the AG News dataset and colored them
according to the four available labels: World, Sports, Business, and
Science/Technology. For each label, a separate color map was generated.
Items belonging to the target label are highlighted in blue, while all
other items are shown in orange. As in previous visualizations, we use a
logarithmic color gradient to represent the number of articles in each
cluster, ranging from white (no articles of that type) to fully saturated
colors (all articles in the cluster belong to the target label). The
resulting trees are shown in Figure \ref{fig-ag-news}. 
\begin{figure*}
  \begin{center}
    \includegraphics[scale=0.8]{ag_news.pdf}
    \caption{The tree obtained from the AG News dataset in different colorations. The tree shown was inferred from the first two principal components derived from SFR-Embedding-Mistral \cite{SFRembedding} embeddings. Parameters yield to the tree building algorithm were: initial $\epsilon = 0.225$, $\Delta\epsilon = 0.0005$, and $\mathrm{minpts} = 5$}
    \label{fig-ag-news}
  \end{center}
\end{figure*}
We observe that the world and sports categories form
clearly separable structures: both branch off early from the root of the
tree, resulting in two well-defined subtrees with only minimal
contamination from other labels. Around the root, however, a small number
of scattered articles appear, suggesting that certain headlines are not
easily classifiable by the embedding models. These headlines seem to be
very distant from the other news items, only joining the root at a
very diffuse density.

The remaining two categories, business and science/technology, show
substantial overlap. They jointly occupy the third major branch of the
tree, and only small terminal sub-branches demonstrate consistent
label-specific structure. We attribute this to the general type of
embedding models that we are using here and that business headlines
frequently mention science and technology and vice versa in this
dataset.

\subsubsection{The French Theses Dataset: Theses-fr-en}

To further evaluate whether our method can identify and organize broad
and fine-grained disciplinary structure, we applied our tree-building
pipeline to the Theses-fr-en dataset, which consists of
English-language abstracts of French doctoral theses. The dataset
contains author-supplied disciplinary annotations, but these labels
are highly heterogeneous and unstandardized, yielding 13,599 unique
descriptors. Therefore, as outlined in
Section \ref{sec-transcription}, we first performed a cross-annotation
step to obtain more tangible labels. 

As an initial experiment, we mapped the original annotations to a
binary classification: humanities versus natural sciences. The
resulting tree clearly separated humanities theses from the remainder
of the corpus, forming a large, coherent branch as shown in Figure
\ref{fig-theses}A. Encouraged by this result, we proceeded to
apply a more detailed transcription based on the OECD Fields of
Science and Technology (FOS) 2002 classification
scheme \cite{oecd-fos}. 

In the tree three major branches are easily identifyable.
The branch identified as “humanities” in the binary test
contains predominantly Social Sciences (5) and Humanities (6)
according to the FOS taxonomy. Social sciences, however, extend beyond
this primary branch, with small isolated scattered clusters appearing
throughout the tree. 

The remaining two major branches contain the Natural Sciences (1), but
with different structural emphases. Strikingly, Engineering and
Technology (2) appears almost exclusively within the very large branch
that occupies nearly the entire upper half of the tree
outlined in Figure \ref{fig-theses}. In contrast, Medical and Health Sciences
(3) predominantly occupy the smaller of the two natural-science
branches, located on the left side of the tree. Examination of FOS
subclasses shows that this medical branch also contains theses
classified as Biological Sciences (1.6), reflecting their conceptual
proximity. 

The pattern of overlaps among FOS categories, particularly the fact
that Natural Sciences (1) span both the engineering-dominant and the
medical-dominant branches, highlights structural ambiguities inherent
in the current FOS hierarchy. Although (1), (2), and (3) are defined
as top-level categories, the tree suggests that Engineering and
Technology (2) and Medical and Health Sciences (3) are more closely
related to different subregions of the Natural Sciences (1) than to
each other. 

Moreover, the tree indicates that (2) and (3) merge into a common
branch earlier than Social Sciences (5) and Humanities (6). After the
social-science and humanities subtree, numerous small and weakly
structured clusters enter the main trunk of the tree (lower right
quadrant), indicating a sizeable number of theses that form islands
in the embedding space and are not well represented by broad
disciplinary groupings. 

All three major branches further subdivide into smaller subtrees
toward their terminal regions. However, we did not find consistent
evidence that these subtrees correspond cleanly to the FOS
subclasses. This lack of correspondence suggests that the tree
structure could support a data-driven reorganizing of disciplinary
taxonomies, potentially capturing conceptual relationships that are
absent, or obscured, in existing classification systems. This
observation directly motivates the subsequent section of the paper. 
\begin{figure*}
  \begin{center}
    \includegraphics[scale=0.8]{theses.pdf}
    \caption{Tree generated from the French Theses theses-fr-en
      dataset using the first two principal components obtained from the embeddings yield by the SFR-Embedding-Mistral \cite{SFRembedding} model:\newline
      \textbf{A}: Coloration according to binary classification Humanities and
      Natural Sciences. \newline
      \textbf{B}: Coloration according to the main groups of the OECD FOS
      classification. \newline
      \textbf{C}: Highlight of the subgroups of the Natural Sciences group
      according to OECD FOS.\newline
      Parameters for tree building were: initial $\epsilon = 0.23$, $\Delta\epsilon = 0.001$, $\mathrm{minpts} = 5$.
    }
    \label{fig-theses}
  \end{center}
\end{figure*}

\subsubsection{Annotated Institutional Trees: TU Wien and AUB}

Our analysis of the two institutional datasets (TU Wien and AUB) differs
from the benchmark datasets presented earlier. In the benchmark cases,
we projected known labels onto the inferred trees to assess how well
embeddings recover an existing classification. In contrast, the
institutional datasets are used to examine research structures ab
initio: rather than testing whether a predefined taxonomy is recovered,
we investigate how the tree itself organizes the institutional research
landscape.
Although the embedding models were taken trained on external corpora,
and not further fine tuned, the
resulting tree structure is independent of any curated ontology. This
allows us to explore institution-specific disciplinary profiles,
structural proximities between fields, and potential emerging clusters
or cross-disciplinary connections. The results demonstrate that our
method can capture meaningful organizational patterns without relying
on predefined categories.
\begin{itemize}

\item  At the American University of Beirut, medicine and health-related
  disciplines dominate the research corpus, reflecting the strong
  presence of the university hospital. This dominance is visible in the
  tree structure, where the entire upper half of the circular tree forms
  the largest and most cohesive subtree. Within this region, several
  well-defined subbranches correspond to specific healthcare domains,
  including health and nutrition, health policy and systems research,
  and public health and epidemiology.

  A distinct humanities subtree is also present, characterized by numerous
  subbranches labeled as Middle Eastern Studies and Conflict
  Resolution. In addition, an engineering subtree that covers engineering,
  physics, and mathematics, is clearly visible. Notably, this engineering
  subtree merges with the humanities subtree at a higher density than with the
  medical subtree, suggesting a closer structural relationship between
  engineering and humanities research topics within this corpus than might
  be expected, which might be characteristic of the university's liberal arts tradition.

  To the left of these major subtrees, smaller clusters related to both
  engineering and humanities fields branch into the tree and eventually
  merge with the broader medical subtree. Beyond this point, several
  near-linear terminal branches from a wide range of disciplines connect
  to the tree as outliers before joining the main structure.
  
  The full tree is shown in Figure \ref{fig-aub-tree} and can be explored
  in detail using our interactive tree viewer at \url{http://}. As in the
  other figures, color intensity is logarithmically scaled from white (no
  articles in the branch) to black (all articles in the dataset).
  
\item Technische Universität Wien (TU Wien)

  The tree structure for TU Wien differs markedly from that of AUB. As a
  technical university, TU Wien has a more focused research portfolio,
  which results in a tree that appears more homogeneous overall.
  Nevertheless, the tree reveals several large and meaningful subtrees that
  highlight the institution’s disciplinary strengths.
  
  At the upper right of the tree, we observe a subtree corresponding to
  quantum mechanics, quantum optics, and quantum-scale materials.
  This region merges smoothly into a broad materials science subtree,
  which occupies much of the upper left quadrant of the circular tree.
  
  Also in the upper left quadrant is a large subtree corresponding to
  information technology and computer science, which subdivides into
  numerous smaller branches. Further branches that merge into the main
  tree at later stages correspond to nanotechnology and semiconductor
  engineering. Additional clusters represent infrastructure planning
  and environmental engineering. One of the earliest large branchings
  near the root corresponds to mathematics and theoretical physics,
  forming a distinct foundational science substructure.

  The TU Wien tree is shown in Figure \ref{fig-tuwien-tree}, and an
  interactive exploration is available through our tree viewer at
  \url{http://}. As with the AUB tree, color intensity follows a
  logarithmic scale from white (no articles) to black (all articles in the
  dataset).

\end{itemize}
\begin{figure*}
  \begin{center}
    \includegraphics{aub-tree.pdf}
    \caption{Tree generated from the AUB dataset: The tree shown was inferred from the first two principal components obtained from Qwen3-Embedding-8B
    \cite{qwen3embedding} model embeddings. The tree was built with the following parameters: initial $\epsilon = 0.0045$,
    $\Delta\epsilon = 0.00001$ and $\mathrm{minpts} = 5$.}
    \label{fig-aub-tree}
  \end{center}
\end{figure*}

\begin{figure*}
  \begin{center}
    \includegraphics{tuwien-tree.pdf}
    \caption{Tree generated from the TU Wien dataset: The tree shown was inferred from the first two principal components obtained from Qwen3-Embedding-8B
    \cite{qwen3embedding} model embeddings. The tree was built with the following parameters: initial $\epsilon = 0.0028$,
    $\Delta\epsilon = 0.000001$ and $\mathrm{minpts} = 5$.}
    \label{fig-tuwien-tree}
  \end{center}
\end{figure*}

We note, as can also be verified using our custom tree viewer, that the
automatic annotation of clusters is not without limitations. In many
cases, the LLM assigns overly general labels—such as engineering—to
large or heterogeneous clusters. Moreover, clusters that are
structurally identical across different annotation runs sometimes
receive different labels, suggesting that the model struggles with
processing extremely large input contexts and exhibits instability in
its naming decisions. Despite these limitations, the annotation process
provided substantial insights into both institutions, their publication
profiles, and the major research fields represented within their
corpora. 

\section{Discussion}

Herein we outline our hierarchical density based tree construction
method, applied onto LLM embeddings from large text corpora. Applied on the
abstracts of scientific publications published by the American
University of Beirut and TU Wien, our method produces interpretable,
multi-resolution trees that reveal how topics, semantic fields, and
research areas are organized relative to each other.  By combining
dimensionality reduction, adaptive multi-scale clustering, and
large-context LLM-based annotation, we establish a pipeline capable of
uncovering latent structure even in datasets lacking standardized or
reliable taxonomies.

\subsection{Insights from Benchmark Datasets}

The benchmark datasets provide a reference for the applicability of
our method.

Application of our method on the 20 newsgroups dataset
revealed that the inferred tree was capable to recover intuitive
relationships such as the proximity of sports-related topics,
technical computing groups and overlaps between religious and
political discussions. The hierarchical approach further uncovered that
sports related discussions are further elongated from the different
groups only merging to the whole tree close its root.

The AG news dataset similarly shows that World and Sports articles
form well separated subtrees, but that Business and Science/Technology
headlines exhibit a substantial overlap. This result can intuitively
explained by the topical similarities between these two
categories. Our tree based method as such creates an intrinsic
semantic organization derived from LLM-embeddings rather then
enforcing a predefined label boundary.

The French doctoral theses dataset provided a further test for our
algorithm. This dataset included ``discipline'' annotations. However
these are rather arbitrarily chosen and unstandarized. We transcribed
these labels first into a binary annotation of either being part of
natural sciences or humanities. To our surprise the theses abstracts
of labeled humanities formed a clear subtree in our tree. Encouraged by
these findings we progressed to transcribe the ``discipine'' labels
into the OECD FOS annotation. We colored the tree according to it and
found thanks to the hierarchical structure that engineering and medical
sciences merge earlier than either social sciences or humanities. A
relationship that as such cannot be derived from the FOS
classification. At the same time we see that finer grained subtrees do
correspond cleanly to the FOS subclasses, indicating that our inferred
structure captures conceptual relationships that differ from
established disciplinary boundaries.

These observations support the idea that our tree-based representations can
serve as a complementary, data-driven alternative to
traditional classification schemes.

The IMDB 50K Reviews dataset yields a very homogeneous tree,
reflecting that the embedding models used in this study
are not able to distinguish the semantic differences between the movie
reviews in a meaningful way. The labels of this dataset are largely
dispersed across the inferred tree, outlining the limitations of our
approach, where the embeddings as outlined here do not encode as here
sentiment or an other required topic separator.   

\subsection{Institution-Specific Research Landscapes}

These datasets are at the core of this research as they illustrate
that using our in here presented pipeline one can derive research
classifications \emph{ab-initio}, under the given that a recent, well
trained LLM-embedding model exists.
In both datasets, from the American University in Beirut and from TU
Wien the inferred trees recover dominant research areas, internal
substructures and relationships between disciplines without relying on
a predefined taxonomy.

At AUB, the strong presence of medical and health-related research
manifests as a large, cohesive subtree branching into public health,
nutrition, epidemiology, and related fields. The emergence of a
well-defined humanities subtree, in particular in Middle Eastern Studies
and Conflict Resolution, highlights the institution’s regional and
cultural focus. The structural proximity between engineering and
humanities observed in the tree suggests interdisciplinary connections
that may not be apparent from administrative classifications alone.

At TU Wien, the tree reflects the institution’s technical orientation,
with prominent subtrees in materials science, quantum physics, information
technology, and engineering. Despite the overall homogeneity of the
corpus, the tree reveals meaningful internal organization.
These findings demonstrate that the method can capture
institution-specific research profiles and internal structure even in
relatively focused corpora.

\subsection{Methodological Considerations}

Several methodological insights emerge from this study. First,
dimensionality reduction via PCA plays a crucial role in enhancing tree
structure. Using $L_2$ distances on a small number of principal
components consistently yielded more articulated and interpretable
trees than cosine distances in the full embedding space. This suggests
that PCA acts as an effective feature-selection mechanism, suppressing
noise and emphasizing the most informative semantic dimensions.

Second, the adaptive multi-$\epsilon$ DBSCAN procedure enables the
recovery of hierarchical structure without imposing a fixed density
threshold. By progressively relaxing density constraints, the method
naturally captures cluster mergers across scales, producing a tree
structure analogous to a dendrogram but grounded in density-based
clustering.

Finally, while LLM-based annotation substantially improves
interpretability, it also introduces limitations. Large clusters often
receive overly general or unstable labels, reflecting current
constraints of long-context inference. However, even imperfect
annotations provide valuable guidance for navigating and interpreting
complex tree structures.

\subsection{Limitations and Future Directions}

The inferred trees are inherently constrained by the representational
capacity of the embedding models. Semantic dimensions that are weakly
encoded, such as fine-grained sentiment or subtle disciplinary
distinctions, cannot be reliably recovered through clustering alone.
Future work could explore domain-specific or task-adapted embedding
models to address this limitation.

In addition, annotation remains computationally expensive and sensitive
to model variability. More robust strategies, such as hierarchical
annotation, or summerization strategies tied together with annotation
strategies may improve stability and scalability.

Looking beyond text-based corpora, a particularly promising direction
is the extension of this framework to other data modalities. The
method itself is agnostic to the origin of the embeddings and could, in
principle, be applied to image embeddings, audio embeddings, or
multimodal representations. For example, trees inferred from image
embeddings could reveal visual themes or stylistic groupings, while
trees built from music embeddings might uncover genre structure,
rhythmic patterns, or harmonic similarities. Applying the proposed
approach to such modalities would allow for the exploration of latent
structure in domains where hierarchical organization is often implicit
rather than explicitly labeled.

\section{Conclusion}
Overall, our results indicate that hierarchical density clustering of
LLM embeddings provides a powerful and flexible framework for mapping
semantic structure in large text corpora. The method
reveals meaningful relationships that both complement and challenge
existing classification systems. By enabling data-driven,
multi-resolution representations of textual domains, this approach
offers new opportunities for research assessment, knowledge mapping,
and other exploratory analysis.

\bibliographystyle{ieeetr}
\bibliography{bib}

\end{document}
